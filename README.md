
# Data Engineering Zoomcamp 2026

Repository for homework and weekly learnings from the [DataTalks.Club Data Engineering Zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp).

## ğŸ“š Project Purpose

A hands-on workspace for completing coursework modules that progressively introduce data engineering concepts and tools. This project focuses on practical implementation of modern data engineering workflows using cloud platforms and orchestration tools, working with real-world NYC taxi data.

## ğŸ—‚ï¸ Repository Structure

```text
de-zoomcamp/
â”œâ”€â”€ 02-workflow-orchestration/     # Module 2: Workflow orchestration with Kestra
â”‚   â”œâ”€â”€ flows/                     # Kestra workflow YAML definitions
â”‚   â”œâ”€â”€ docker-compose.yaml        # Kestra + PostgreSQL services setup
â”‚   â””â”€â”€ .env_encoded               # Encrypted environment variables for GCP credentials
â”‚
â”œâ”€â”€ homework-01/                   # Module 1: Docker/Postgres/Terraform/GCP
â”‚   â”œâ”€â”€ homework_notebook.ipynb    # Solutions to homework questions
â”‚   â””â”€â”€ README.md                  # Question answers and SQL queries
â”‚
â”œâ”€â”€ homework-02/                   # Module 2: Workflow Orchestration
â”‚   â”œâ”€â”€ flows/                     # Kestra flows for data ingestion
â”‚   â”œâ”€â”€ queries.sql                # BigQuery queries for verification
â”‚   â””â”€â”€ README.md                  # Homework answers and explanations
â”‚
â”œâ”€â”€ homework-03/                   # Module 3: Data Warehouse (BigQuery)
â”‚   â””â”€â”€ README.md                  # BigQuery optimization and table creation queries
â”‚
â”œâ”€â”€ pipeline/                      # Core data pipeline with local development setup
â”‚   â”œâ”€â”€ ingest_data.py             # CLI tool for ingesting NYC taxi data to PostgreSQL
â”‚   â”œâ”€â”€ pipeline.py                # Simple pipeline script generating parquet files
â”‚   â”œâ”€â”€ docker-compose.yaml        # PostgreSQL + PgAdmin services
â”‚   â”œâ”€â”€ Dockerfile                 # Containerized pipeline execution
â”‚   â””â”€â”€ pyproject.toml             # Project dependencies (uv-based)
â”‚
â””â”€â”€ test/                          # Basic testing utilities
```

## ğŸ”§ Technologies & Tools

**Languages:** Python 3.13, SQL, YAML
**Orchestration:** Kestra v1.1
**Databases:** PostgreSQL 18, Google BigQuery
**Cloud:** Google Cloud Platform (GCS, BigQuery)
**Containerization:** Docker, Docker Compose
**Python Stack:** pandas, sqlalchemy, pyarrow, click, tqdm

## Course Modules

### **Module 1**: Docker, Postgres, Terraform, GCP
- Containerized data ingestion pipeline
- PostgreSQL database setup with PgAdmin
- Infrastructure as Code with Terraform
- GCP project and service account configuration
- **Key Files:** [pipeline/ingest_data.py](pipeline/ingest_data.py), [homework-01/](homework-01/)

### **Module 2**: Workflow Orchestration (Kestra)
- Declarative workflow definitions in YAML
- Scheduled data ingestion pipelines
- GCP integration (GCS upload, BigQuery table creation)
- Environment variable management and credential handling
- **Key Files:** [02-workflow-orchestration/flows/](02-workflow-orchestration/flows/), [homework-02/](homework-02/)

### **Module 3**: Data Warehouse (BigQuery)
- External tables pointing to GCS files
- Materialized tables for query optimization
- Partitioning strategies (by datetime)
- Clustering for improved query performance
- Cost optimization techniques
- **Key Files:** [homework-03/](homework-03/)

### **Module 4**: Analytics Engineering (dbt)
- Transforming data in the warehouse
- Building data models and tests
- Documentation and lineage

### **Module 5**: Batch Processing (Spark)
- Distributed data processing
- PySpark fundamentals
- Performance optimization

### **Module 6**: Stream Processing (Kafka)
- Real-time data ingestion
- Event-driven architectures
- Stream processing patterns

### **Module 7**: Final Project
- End-to-end data pipeline implementation

## ğŸš€ Main Components

### 1. Data Ingestion Pipeline
- Downloads NYC taxi data from GitHub releases
- Loads into PostgreSQL using chunked processing (100K rows/chunk)
- CLI-based with flexible year/month selection
- Progress tracking with tqdm

### 2. Workflow Orchestration
- Kestra workflows from basic hello-world to GCP-integrated pipelines
- Features: templating, scheduling, conditional execution
- Automated GCS uploads and BigQuery table creation

### 3. Data Warehouse
- BigQuery external and materialized tables
- Query optimization with partitioning and clustering
- Cost-effective data storage and retrieval strategies

## ğŸ“Š Architecture Flow

```
NYC Taxi Data (GitHub)
    â†“
Local PostgreSQL (development)
    â†“
Kestra Orchestration (workflows)
    â†“
Google Cloud Storage (data lake)
    â†“
BigQuery (data warehouse)
    â†“
Optimized queries (partitioned/clustered)
```

## Progress

- [x] Module 1 - Docker, Postgres, Terraform, GCP
- [x] Module 2 - Workflow Orchestration (Kestra)
- [x] Module 3 - Data Warehouse (BigQuery)
- [ ] Module 4 - Analytics Engineering (dbt)
- [ ] Module 5 - Batch Processing (Spark)
- [ ] Module 6 - Stream Processing (Kafka)
- [ ] Module 7 - Final Project

## ğŸ¯ Learning Outcomes

Through this repository, I've gained hands-on experience with:
- Containerizing data applications with Docker
- Building data ingestion pipelines with Python
- Orchestrating workflows with modern tools (Kestra)
- Managing cloud infrastructure on GCP
- Optimizing data warehouses for cost and performance
- Working with real-world datasets at scale

## ğŸ”— Resources

- [Course Repository](https://github.com/DataTalksClub/data-engineering-zoomcamp)
- [DataTalks.Club](https://datatalks.club/)
- [Kestra Documentation](https://kestra.io/docs)
- [Google BigQuery Documentation](https://cloud.google.com/bigquery/docs)
